# Lab 7: Migrate data to cloud with AzCopy

**Duration:** 45m
---
---

## 🔧 Azure Lab Instructions: Migrate Data to Cloud with AzCopy

**Duration:** 45 minutes
**Objective:** Upload and sync local files to Azure Blob Storage using AzCopy CLI.

---

### After logging in with your credentials:

---

### **Step 1: Create a Storage Account**

1. In the Azure portal, search for **Storage accounts** and select it.

2. Click **+ Create**.

3. Under the **Basics** tab:

   * **Subscription**: Use default/available subscription
   * **Resource group**: Select or create **`rg-learntech-naveed`**
   * **Storage account name**: Enter **`ncloudstorage01`** (must be globally unique)
   * **Region**: **East US**
   * **Performance**: **Standard**
   * **Redundancy**: **Geo-redundant storage (GRS)**

4. Go to the **Advanced** tab:

   * Set **Hierarchical namespace** to **Enabled**

5. Click **Review + create** → Click **Create**

6. After deployment, click **Go to resource**

7. In the left menu, click **Containers** → then **+ Container**

8. Name the container **`democontainer-naveed`** → Click **Create**

---

### **Step 2: Download and Configure AzCopy**

1. Download AzCopy from: [https://aka.ms/downloadazcopy-v10-windows](https://aka.ms/downloadazcopy-v10-windows)

2. Extract the downloaded `.zip` to **`C:\Tools\AzCopy`**

3. Open **Command Prompt** and navigate to the AzCopy folder:

   ```bash
   cd C:\Tools\AzCopy
   ```

4. Authenticate AzCopy:

   ```bash
   azcopy login
   ```

5. Open the browser, enter the code shown in terminal, and sign in with your Azure credentials.

---

### **Step 3: Upload Files to Azure Blob Storage**

1. Prepare a local folder, e.g., **`C:\DataToUpload`**, and add sample files.

2. Use the following AzCopy command to copy all files:

   ```bash
   azcopy copy "C:\DataToUpload" "https://ncloudstorage01.blob.core.windows.net/democontainer-naveed" --recursive=true
   ```

3. In the Azure portal, go to the container to verify the uploaded files.

---

### **Step 4: Modify and Sync Data for Testing**

1. Modify or add a file in **`C:\DataToUpload`**

2. Sync the folder to the Azure container:

   ```bash
   azcopy sync "C:\DataToUpload" "https://ncloudstorage01.blob.core.windows.net/democontainer-naveed" --recursive=true
   ```

3. Confirm the changes reflect in the container.

---

### **Step 5: Create a Scheduled Task for Automated Sync**

1. Open **Notepad**, paste the sync command, and update paths as needed:

   ```bash
   azcopy sync "C:\DataToUpload" "https://ncloudstorage01.blob.core.windows.net/democontainer-naveed" --recursive=true
   ```

2. Save the file as:
   **`C:\Scripts\sync-ncloudedge.bat`**

3. Open **Command Prompt as Administrator** and create a scheduled task:

   ```bash
   schtasks /CREATE /SC minute /MO 5 /TN "AzCopy-Sync-NCloudEdge" /TR C:\Scripts\sync-ncloudedge.bat
   ```

4. Add or update a file in the folder and wait for the scheduled task to sync it automatically.

---

### **Step 6: Final Step**

**Delete all the resources.**

---

This lab follows a real-world DevOps-style process of using **command-line tools** for **efficient, repeatable blob storage management**, simulating tasks performed by **Cloud Administrators** or **Storage Engineers** in professional infrastructure environments.

---
---
---

## 🧾 **Structured Lab Summary: Migrate Data to Cloud with AzCopy**

---

### 🔹 **Purpose of the Lab**

In this lab, **Naveed**, an aspiring cloud engineer at **LearnTechCloud**, learns how to use **AzCopy**, a powerful command-line utility, to migrate data from an on-premises folder to **Azure Blob Storage**. This exercise simulates a real-world scenario where companies want to offload local data to the cloud for durability, global access, and cost-effective storage.

The lab walks through:

* Creating a **Storage Account**
* Uploading files to a **Blob container**
* Synchronizing updates between local files and Azure
* Automating file syncing using **Windows Task Scheduler**

The objective is to provide hands-on experience with **AzCopy CLI**, preparing **Naveed** for real DevOps workflows and cloud migration tasks.

---

### 🧰 **Azure Tools and Services Used**

Here is a list of the core **Azure tools and features** utilized in this lab, including their definitions and role in the workflow:

---

#### 1. **Azure Storage Account**

* **Definition:** A container for storing blobs, files, queues, and tables.
* **Role in Lab:** **Naveed** creates a **Storage Account** named `ncloudstorage01` to serve as the destination for blob uploads.
* **Example:** `stlearntechnaveed` or `ncloudstorage01`

---

#### 2. **Blob Container**

* **Definition:** A logical grouping of blobs (objects/files) within a storage account.
* **Role in Lab:** **The CloudOps engineer** creates a container named `democontainer-naveed` to organize uploaded files.
* **Example:** `democontainer-naveed` used for project sync.

---

#### 3. **AzCopy CLI**

* **Definition:** A lightweight command-line tool by Microsoft for transferring data to and from Azure Storage.
* **Role in Lab:** Used to upload and sync local files with Azure Blob Storage. It offers high-speed, scriptable transfer operations that are ideal for automation.
* **Example Command:**

  ```bash
  azcopy copy "C:\DataToUpload" "https://ncloudstorage01.blob.core.windows.net/democontainer-naveed" --recursive=true
  ```

---

#### 4. **Windows Task Scheduler**

* **Definition:** A built-in Windows utility for automating task execution at specified times or intervals.
* **Role in Lab:** **The Curious Cloud Explorer** uses it to create a scheduled task that automatically runs the AzCopy sync command every 5 minutes, simulating a real-time backup/sync pipeline.
* **Example Command:**

  ```bash
  schtasks /CREATE /SC minute /MO 5 /TN "AzCopy-Sync-NCloudEdge" /TR C:\Scripts\sync-ncloudedge.bat
  ```

---

#### 5. **Command Prompt (CMD) / PowerShell**

* **Definition:** Native Windows terminal interfaces used to run administrative scripts and tools.
* **Role in Lab:** Used by **the TechWaveNaveed admin** to run AzCopy commands and set up the scheduled task.

---

### 🔎 **Real-World Use Case Example**

**Scenario:**
A mid-sized IT services company, **CloudLabX**, needs to back up weekly reports from its internal file server to Azure for disaster recovery and centralized access. Their cloud engineer **Naveed** uses AzCopy to automate the upload and sync of reports to `democontainer-naveed`, hosted in the `ncloudstorage01` account. He configures a **batch script** and **Task Scheduler** to automate the process.

This setup ensures:

* Data resiliency through **Geo-redundant storage**
* Minimal manual overhead using **automation**
* Auditability and visibility via the Azure portal

---

This lab is essential for learners and professionals preparing for the **AZ-104** certification and real-world DevOps roles. It builds muscle memory for handling **cloud data migrations**, **automation**, and **hybrid storage management** — core responsibilities of a modern **Azure Administrator**.

---
---
---

## 🌐 Practical Scenario: Migrating On-Prem Files to Azure with AzCopy

**Naveed**, a skilled but ever-curious **CloudOps engineer** at **LearnTechCloud**, had just been handed a task critical to a client at **SkyStack Labs**. The client needed to **move operational documents**—project reports, system logs, and internal archives—from a local server in their data center to the **Azure cloud** for secure storage, disaster recovery, and remote team access.

The IT leadership at **SkyStack Labs** also wanted the process to be **automated**, ensuring that any new files added locally would be **synced** to Azure on a regular basis. Naveed knew this was a classic case for using **AzCopy**, a fast, lightweight command-line tool designed for exactly this purpose.

---

### 🔧 Step-by-Step Execution with Business Logic

#### **Step 1: Setting Up the Storage Infrastructure**

To begin, **the Curious Cloud Explorer** logged in to the **Azure portal** and created a new **Storage Account** named `stlearntechnaveed` within the resource group `rg-learntech-naveed`. He selected the **East US** region and configured it with **Geo-redundant storage (GRS)** to ensure maximum data resiliency—even if a datacenter failed, a backup would exist in another region.

> 💼 *Why this matters:* This setup provides high availability for files and supports enterprise-grade durability.

He then created a **Blob container** named `democontainer-naveed` to organize and logically group the uploaded files.

---

#### **Step 2: Installing and Authenticating AzCopy**

On his local machine, the **TechWaveNaveed admin** downloaded **AzCopy v10**, extracted it, and opened **Command Prompt**. He navigated to the AzCopy folder using:

```bash
cd C:\Tools\AzCopy
```

He authenticated securely using:

```bash
azcopy login
```

A browser popped up asking him to paste a login code. After logging in with his Azure credentials, AzCopy was ready for action.

> 🔐 *Why this matters:* Secure authentication ensures AzCopy only connects to authorized Azure resources.

---

#### **Step 3: Uploading Files to Azure Blob Storage**

He prepared a local folder `C:\DataToUpload` containing PDFs, Excel files, and configuration scripts.

To copy these to Azure, he ran:

```bash
azcopy copy "C:\DataToUpload" "https://stlearntechnaveed.blob.core.windows.net/democontainer-naveed" --recursive=true
```

After a short moment, he saw confirmation in the terminal—and checking the **Azure Portal**, all files were there.

> 📁 *Why this matters:* Rapid upload with retry mechanisms made this faster and more resilient than GUI or drag-and-drop uploads.

---

#### **Step 4: Syncing Updates**

Next, Naveed edited a file locally and added a new one. To reflect those changes in Azure without re-uploading everything, he ran:

```bash
azcopy sync "C:\DataToUpload" "https://stlearntechnaveed.blob.core.windows.net/democontainer-naveed" --recursive=true
```

Only the changes were pushed, saving time and bandwidth.

> 🔄 *Why this matters:* Syncing reduces transfer time, ideal for real-time or frequent backup scenarios.

---

#### **Step 5: Automating the Sync with Task Scheduler**

To automate the sync every 5 minutes, the **CloudOps engineer** created a `.bat` script:

```batch
azcopy sync "C:\DataToUpload" "https://stlearntechnaveed.blob.core.windows.net/democontainer-naveed" --recursive=true
```

He saved it as `sync-ncloudedge.bat` and scheduled it using:

```bash
schtasks /CREATE /SC minute /MO 5 /TN "AzCopySyncNaveed" /TR "C:\Scripts\sync-ncloudedge.bat"
```

Now, his sync would run every 5 minutes automatically.

> 🤖 *Why this matters:* Automating backups ensures no human error or delays during high-availability workflows.

---

#### **Step 6: Wrapping Up**

Once the task was tested and data verified, the **NextGenInfra technician** deleted all Azure resources to avoid unnecessary charges.

---

### ✅ Real-World Value & Relevance

This lab represents a real scenario faced by companies migrating from **on-premises to cloud** infrastructure. Businesses like **TheOpsFactory** and **NCloudEdge** often look for professionals who can not only set up Azure resources but also **script**, **secure**, and **automate data flows**. Mastery of tools like **AzCopy**, combined with practical automation using **Task Scheduler**, gives engineers like **Naveed** a professional edge in the competitive Azure job market.

---
---
---


## **Story Scenario: Migrating Marketing Team Archives to Azure Blob Storage Using AzCopy**

**Character**: *Emily Roberts*, an IT Systems Engineer at a mid-sized marketing agency in Chicago.

### **Background**

Emily’s marketing agency recently moved to a hybrid cloud infrastructure to improve scalability and cost-efficiency. The creative team has years’ worth of campaign files—videos, graphics, documents—stored on an on-premises file server. With increasing remote collaboration and compliance requirements, management asked Emily to **migrate these assets securely and reliably to the cloud**.

Her goal is to **store these files in Azure Blob Storage**, ensure **efficient future syncs** for updated content, and implement **automated data transfer** on a regular schedule.

---

## **Step-by-Step Journey**

### **Step 1: Creating the Cloud Storage Environment**

Emily starts by logging into the **Azure Portal**, where she searches for **Storage Accounts** and clicks **+ Create**. She selects the appropriate **resource group** and names her **storage account** something unique like `marketingfilesemily`.

She selects **East US** as the **region**, sets the **performance** to **Standard**, and chooses **Geo-redundant storage (GRS)** to ensure data durability across regions.

Because the files will be organized in a directory-like structure, she enables the **Hierarchical Namespace (HNS)** option under the **Advanced** tab, turning her storage into a **Data Lake Storage Gen2-compatible account**.

Once deployment is complete, Emily navigates to the **Containers** section of the storage account and creates a **new container** named `creative-assets`.

> 📌 **Why it matters**: Creating a **Storage Account** with **Blob Storage** and enabling **HNS** provides scalable, secure, and structured cloud storage ideal for media-heavy departments like marketing.

---

### **Step 2: Preparing and Uploading with AzCopy**

To migrate the data, Emily decides to use **AzCopy**, a fast and lightweight **command-line tool** optimized for data transfers to Azure.

She downloads **AzCopy** from the official link, extracts it, and opens **Command Prompt**. She navigates to the extracted folder using the `cd` command.

To connect **AzCopy** to her Azure account, she runs:

```bash
azcopy login
```

This opens a browser prompt where she enters the provided code and logs in securely.

She then runs the following command to **upload the files** from her local `D:\MarketingArchives` folder to her new **Blob container**:

```bash
azcopy copy "D:\MarketingArchives" "https://marketingfilesemily.blob.core.windows.net/creative-assets" --recursive=true
```

> 📌 **Why it matters**: Using **AzCopy** ensures high-speed, scriptable, and resumable uploads—essential for large file sets or unreliable networks.

---

### **Step 3: Syncing New Files**

A few days later, the creative team adds new content to the local folder. Instead of re-uploading everything, Emily uses the **sync** command to only upload new or changed files:

```bash
azcopy sync "D:\MarketingArchives" "https://marketingfilesemily.blob.core.windows.net/creative-assets" --recursive=true
```

> 📌 **Why it matters**: The **AzCopy sync** feature improves efficiency by skipping already uploaded files, reducing bandwidth usage and transfer time.

---

### **Step 4: Automating Future Transfers**

Emily wants to automate this process so updates are pushed to Azure every 5 minutes. She creates a **batch script** (`sync_script.bat`) with the sync command and saves it to `C:\Scripts\`.

Using **Windows Task Scheduler**, she runs this command:

```bash
schtasks /CREATE /SC minute /MO 5 /TN "AzCopySyncTask" /TR C:\Scripts\sync_script.bat
```

> 📌 **Why it matters**: Automation via **Task Scheduler** ensures timely, consistent syncs without manual intervention, supporting business continuity and compliance.

---

### **Outcome and Business Value**

Emily verifies in the **Azure Portal** that all files are present in the **Blob container**. The marketing team can now access campaign files from anywhere using Azure-based tools or custom applications.

This setup ensures:

* **Data availability for remote teams**
* **Centralized cloud-based archival**
* **Reduced risk of local server failure**
* **Secure, auditable, and scalable storage**

---

## **Conclusion**

Emily successfully migrated legacy assets to **Azure Blob Storage** using **AzCopy**, implemented **efficient syncing**, and automated the process via **Task Scheduler**. This lab mirrors real-world enterprise needs and showcases how to use **Azure Storage**, **AzCopy**, and **automation tools** to solve critical business challenges.

---
## Lab-Based Conceptual MCQs

### 1. What is the primary benefit of using a **Private Endpoint** when configuring a **Storage Account** in Azure?

**(a)** To enable replication across regions  
**(b)** To access the storage account over a secure private IP  
**(c)** To create multiple containers  
**(d)** To allow anonymous access  

**✅ Correct answer: (b)**  
**Explanation**: A **Private Endpoint** ensures secure access to Azure services using a private IP address within your virtual network, eliminating exposure to the public internet.

---

### 2. Why is it important to associate the **Private Endpoint** with a **Virtual Network** and **Subnet**?

**(a)** To enable backup  
**(b)** To allocate public IP addresses  
**(c)** To enforce network isolation and control  
**(d)** To reduce VM provisioning time  

**✅ Correct answer: (c)**  
**Explanation**: Associating the **Private Endpoint** with a **Virtual Network** and **Subnet** enforces network segmentation and prevents exposure to external threats.

---

### 3. What is the role of the **Storage Explorer** tool in the lab?

**(a)** Monitor network latency  
**(b)** Encrypt VM disks  
**(c)** Interact with blob storage using a GUI  
**(d)** Assign RBAC roles  

**✅ Correct answer: (c)**  
**Explanation**: **Azure Storage Explorer** provides a graphical interface to manage and interact with blob containers, file shares, and queues in a storage account.

---

### 4. What does the `nslookup` command verify in the virtual machine?

**(a)** VM disk size  
**(b)** Azure DNS resolution of the **Storage Account**  
**(c)** Subscription ID  
**(d)** Network performance  

**✅ Correct answer: (b)**  
**Explanation**: Running `nslookup` helps confirm that the **Storage Account** resolves to a private IP, verifying that the **Private Endpoint** is working as expected.

---

### 5. Why is **IE Enhanced Security Configuration** turned off in the virtual machine?

**(a)** To reduce firewall rules  
**(b)** To improve browser performance  
**(c)** To enable downloading and installing tools like **Storage Explorer**  
**(d)** To improve IP resolution  

**✅ Correct answer: (c)**  
**Explanation**: Disabling **IE Enhanced Security Configuration** allows downloads and installation of tools, which would otherwise be blocked due to security restrictions.

---

### 6. What does enabling **Hierarchical Namespace** allow in the **Storage Account**?

**(a)** Redundant storage replication  
**(b)** Virtual network integration  
**(c)** Advanced access controls for blob objects  
**(d)** Directory-like structure for blob storage  

**✅ Correct answer: (d)**  
**Explanation**: Enabling **Hierarchical Namespace** allows blobs to be organized in a directory structure, supporting features like ACLs and optimized file operations.

---

### 7. Which type of Azure redundancy offers protection against regional outages?

**(a)** LRS  
**(b)** ZRS  
**(c)** GRS  
**(d)** None  

**✅ Correct answer: (c)**  
**Explanation**: **Geo-Redundant Storage (GRS)** replicates data to a secondary region, providing durability even in the case of a regional failure.

---

### 8. What is the purpose of using **Standard SSD** for VM disk configuration?

**(a)** To enable live migration  
**(b)** To reduce costs while maintaining moderate performance  
**(c)** To automatically encrypt the OS disk  
**(d)** To allow VM snapshots  

**✅ Correct answer: (b)**  
**Explanation**: **Standard SSD** provides a cost-effective disk solution with balanced performance, ideal for workloads that don’t require high throughput.

---

### 9. Why is **RDP** access configured when creating the virtual machine?

**(a)** To allow browser-based access  
**(b)** To allow command-line SSH  
**(c)** To allow remote desktop connectivity for GUI-based operations  
**(d)** To avoid using credentials  

**✅ Correct answer: (c)**  
**Explanation**: **Remote Desktop Protocol (RDP)** is configured to allow users to connect to the Windows VM and interact via its graphical user interface.

---

### 10. What does disabling **public access** on a **Storage Account** help achieve?

**(a)** Faster uploads  
**(b)** Increased availability  
**(c)** Enhanced security by restricting access to private networks only  
**(d)** More regions supported  

**✅ Correct answer: (c)**  
**Explanation**: Disabling **public access** ensures the **Storage Account** can only be accessed via **Private Endpoints**, thus enhancing network security.


